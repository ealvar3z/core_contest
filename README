NAME

Hallucination-nonidentifiability: Structural Impossibility Result for Next-token Models

SYNOPSIS

A proof that hallucination is unavoidable in next-token language models under
text-only learning, underspecified prompts, and forced emission.

DESCRIPTION

This repository contains a self-contained structural argument showing that,
when the underlying world state is non-identifiable from text and the system
must act, false factual assertions are unavoidable.

The result is independent of scale, optimization quality, or architecture.

BUILD

Requirements:

- pandoc

```
pandoc paper.org -f org -t gfm --wrap=preserve -o paper.md
```

SCOPE

This work does not (**yet**):
- benchmark models
- propose mitigations
- analyze architectures
- discuss alignment policy
- model human cognition

LICENSE

See LICENSE if present.
